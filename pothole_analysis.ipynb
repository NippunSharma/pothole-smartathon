{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smartathon\n",
    "### Complete Pothole Analysis from Visual Input\n",
    "#### Team Name: akatsuki\n",
    "#### Team Member: Nippun Sharma\n",
    "\n",
    "Email: [inbox.nippun@gmail.com](inbox.nippun@gmail.com)\n",
    "\n",
    "### Introduction\n",
    "The problem provided is to accurately detect and localize potholes in a given area, just by using a video input stream from\n",
    "a moving car. The challenge itself becomes complicated as we are only allowed to use the video frames to perform any sort of\n",
    "predictions. In this document, I will propose an automated way for detecting, locating, and possibly reconstructing potholes\n",
    "just using the video input. As a proof of concept (PoC), I will also apply most of the discussed techniques on the example video\n",
    "file provided by the organizers.\n",
    "\n",
    "\n",
    "### Detection\n",
    "The most straight-forward part of this challenge was to detect potholes i.e. create bounding-boxes around potholes in the\n",
    "video. I used a Yolov5m (medium) model to perform this task. The model was trained on the 2022 version of the Road Damage\n",
    "Detection Dataset. This dataset consists Potholes, Longitudinal Cracks, Transverse Cracks and Aligator Cracks.\n",
    "\n",
    "[This](https://github.com/sekilab/RoadDamageDetector) is the link to github repository of the RDD Dataset.\n",
    "\n",
    "### Tracking\n",
    "In our case we also want to count the total number of unique potholes that were visible in the entire journey.\n",
    "A normal detector will not provide us with unique boxes, as a pothole detected in one frame will also get detected in the next one.\n",
    "Thus counting the total number of detections will lead to a double-counting problem. To tackle this problem, I used detection with\n",
    "tracking. Tracking is the process of assigning a unique identifier to a bounding-box and keeping sure that the box has the same\n",
    "identifier in all subsequent frames. Tracking will prevent the double-counting and we will be able to count the actual number of\n",
    "potholes. I used SORT (Simple Online and Realtime Tracking), which is a computer-vision based algorithm to track bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.config.config import config\n",
    "from modules.detection.detection import apply_detection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from sort.sort import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_VIDEO = \"demo/sections.mov\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "YOLO_DIR = \"./yolov5\"\n",
    "DETECTION_MODEL = \"./pretrained_model/yolov5.onnx\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "if not os.path.exists(Path(RESULTS_DIR) / \"detection\"):\n",
    "    os.mkdir(Path(RESULTS_DIR) / \"detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-1-17 Python-3.9.15 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Loading pretrained_model\\yolov5.onnx for ONNX Runtime inference...\n",
      "Adding AutoShape... \n",
      "100%|██████████| 3844/3844 [06:31<00:00,  9.81it/s] \n",
      "100%|██████████| 3844/3844 [03:42<00:00, 17.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply detection and tracking on the dummy video here.\n",
    "# output saved in results/detection/section.mp4\n",
    "\n",
    "cap = cv2.VideoCapture(DEMO_VIDEO)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    logging.error(f\"Could not open video file {DEMO_VIDEO}\")\n",
    "    raise\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "out = cv2.VideoWriter((Path(RESULTS_DIR) / \"detection\" / f\"{Path(DEMO_VIDEO).stem}.mp4\").as_posix(),\n",
    "    cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
    "\n",
    "detections_df = pd.DataFrame(columns=[\"id\", \"frame\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class\"])\n",
    "\n",
    "num_frames = 0\n",
    "\n",
    "# object trackers.\n",
    "pothole_tracker = Sort(max_age=10, min_hits=3)\n",
    "pothole_ids = []\n",
    "\n",
    "long_crack_tracker = Sort(max_age=10, min_hits=3)\n",
    "long_crack_ids = []\n",
    "\n",
    "trans_crack_tracker = Sort(max_age=10, min_hits=3)\n",
    "trans_crack_ids = []\n",
    "\n",
    "alig_crack_tracker = Sort(max_age=10, min_hits=3)\n",
    "alig_crack_ids = []\n",
    "\n",
    "model = torch.hub.load(YOLO_DIR, 'custom', DETECTION_MODEL, source='local')\n",
    "pbar = tqdm(total=length)\n",
    "\n",
    "# iterate over all video frames.\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # perform detection.\n",
    "        result = apply_detection(frame, model)\n",
    "\n",
    "        if result.shape[0] == 0:\n",
    "            detections = np.empty((0,5))\n",
    "            pothole_tracks = pothole_tracker.update(detections)\n",
    "            long_crack_tracks = long_crack_tracker.update(detections)\n",
    "            trans_crack_tracks = trans_crack_tracker.update(detections)\n",
    "            alig_crack_tracks = alig_crack_tracker.update(detections)\n",
    "        else:\n",
    "            detections = result.loc[:, [\"xmin\",\"ymin\",\"xmax\",\"ymax\",\"confidence\"]]\n",
    "            detections[[\"xmin\", \"xmax\"]] = detections[[\"xmin\", \"xmax\"]] * 1920 / 640\n",
    "            detections[[\"ymin\", \"ymax\"]] = detections[[\"ymin\", \"ymax\"]] * 1080 / 640\n",
    "\n",
    "            potholes = detections.loc[result[\"name\"] == \"D40\", :].values\n",
    "            pothole_tracks = pothole_tracker.update(potholes)\n",
    "\n",
    "            long_cracks = detections.loc[result[\"name\"] == \"D00\", :].values\n",
    "            long_crack_tracks = long_crack_tracker.update(long_cracks)\n",
    "\n",
    "            trans_cracks = detections.loc[result[\"name\"] == \"D10\", :].values\n",
    "            trans_crack_tracks = trans_crack_tracker.update(trans_cracks)\n",
    "\n",
    "            alig_cracks = detections.loc[result[\"name\"] == \"D20\", :].values\n",
    "            alig_crack_tracks = alig_crack_tracker.update(alig_cracks)\n",
    "\n",
    "        for track in pothole_tracks:\n",
    "            bbox = track[:4].astype(int)\n",
    "            track_id = track[-1]\n",
    "\n",
    "            if track_id not in pothole_ids:\n",
    "                pothole_ids.append(track_id)\n",
    "\n",
    "            detections_df.loc[len(detections_df)] = [pothole_ids.index(track_id), num_frames, track[0], track[1], track[2], track[3], \"D40\"]\n",
    "\n",
    "            cv2.rectangle(frame, bbox[:2], bbox[2:], (0,0,255), 2)\n",
    "            cv2.putText(frame, f\"ID: {pothole_ids.index(track_id)} - pothole\", (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "        for track in long_crack_tracks:\n",
    "            bbox = track[:4].astype(int)\n",
    "            track_id = track[-1]\n",
    "\n",
    "            if track_id not in long_crack_ids:\n",
    "                long_crack_ids.append(track_id)\n",
    "\n",
    "            detections_df.loc[len(detections_df)] = [long_crack_ids.index(track_id), num_frames, track[0], track[1], track[2], track[3], \"D00\"]\n",
    "\n",
    "            cv2.rectangle(frame, bbox[:2], bbox[2:], (0,255,0), 2)\n",
    "            cv2.putText(frame, f\"ID: {long_crack_ids.index(track_id)} - long. crack\", (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "        for track in trans_crack_tracks:\n",
    "            bbox = track[:4].astype(int)\n",
    "            track_id = track[-1]\n",
    "\n",
    "            if track_id not in trans_crack_ids:\n",
    "                trans_crack_ids.append(track_id)\n",
    "\n",
    "            detections_df.loc[len(detections_df)] = [trans_crack_ids.index(track_id), num_frames, track[0], track[1], track[2], track[3], \"D10\"]\n",
    "\n",
    "            cv2.rectangle(frame, bbox[:2], bbox[2:], (255,0,0), 2)\n",
    "            cv2.putText(frame, f\"ID: {trans_crack_ids.index(track_id)} - trans. crack\", (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "        for track in alig_crack_tracks:\n",
    "            bbox = track[:4].astype(int)\n",
    "            track_id = track[-1]\n",
    "\n",
    "            if track_id not in alig_crack_ids:\n",
    "                alig_crack_ids.append(track_id)\n",
    "\n",
    "            detections_df.loc[len(detections_df)] = [alig_crack_ids.index(track_id), num_frames, track[0], track[1], track[2], track[3], \"D20\"]\n",
    "\n",
    "            cv2.rectangle(frame, bbox[:2], bbox[2:], (0,0,0), 2)\n",
    "            cv2.putText(frame, f\"ID: {alig_crack_ids.index(track_id)} - alig. crack\", (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"POTHOLE COUNT: {len(pothole_ids)}\", (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,0), 2)\n",
    "        cv2.putText(frame, f\"LONG. CRACK COUNT: {len(long_crack_ids)}\", (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,0), 2)\n",
    "        cv2.putText(frame, f\"TRANS. CRACK COUNT: {len(trans_crack_ids)}\", (40, 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,0), 2)\n",
    "        cv2.putText(frame, f\"ALIG. CRACK COUNT: {len(alig_crack_ids)}\", (40, 160), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,0), 2)\n",
    "\n",
    "        pbar.update(1)\n",
    "        num_frames += 1\n",
    "        out.write(frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "detections_df.to_csv(Path(RESULTS_DIR) / \"detection\" / \"detections.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `results/detection` for the generated video file (`sections.mp4`) with bounding boxes and counts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Scale\n",
    "It is well known that the structure-of-motion from a single camera only results in a reconstruction up to a scale. Meaning that, there\n",
    "is no sense of absolute distances (such as metres or centi-meters), all points are relatively placed up to a scale. However, in our\n",
    "case, it is required to measure the actual dimensions of the pothole and find its absolute location within the complete journey\n",
    "travelled by the vehicle. Our problem falls under a special category of SFM where our vehicle puts certain constraints on the camera\n",
    "motion, that are known as non-holonomic constraints. Especially, when the camera is at an offset w.r.t. the car's center of gravity.\n",
    "The solution is based on the physics behind the instantaneous center of rotation (ICR). Basically, any moving object can be considered\n",
    "as rotating about its ICR.\n",
    "\n",
    "For an in-depth analysis into the setup and solution, you can read this very interesting [paper](https://rpg.ifi.uzh.ch/docs/ICCV09_scaramuzza.pdf) by Davide Scaramuzza. Below is the code that I have written after reading the paper and I use it for generating an approximate absolute\n",
    "scale value. I have used the least-squares version, which is a 3-point algorithm. Also, as a quick demonstration (when we plot using visual\n",
    "odometry) I have extracted a subset of frames from the demo video where the car is turning around the corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_theta_phi(image_1_pts, image_2_pts):\n",
    "    # find theta and phi angles from image correspondences.\n",
    "    # make sure that there are at-least 3 corresponding pairs.\n",
    "\n",
    "    N = image_1_pts.shape[0]\n",
    "    A = np.zeros((N,4), dtype=float)\n",
    "\n",
    "    A[:,0] = image_1_pts[:,0] * image_2_pts[:,1]\n",
    "    A[:,1] = image_1_pts[:,1] * image_2_pts[:,0]\n",
    "    A[:,2] = image_1_pts[:,2] * image_2_pts[:,1]\n",
    "    A[:,3] = image_1_pts[:,1] * image_2_pts[:,2]\n",
    "\n",
    "    U, S, V = np.linalg.svd(A)\n",
    "    result = V[:,-1]\n",
    "\n",
    "    phi = np.arctan2(result[2], -result[0])\n",
    "    theta = phi + np.arctan2(result[3], result[1])\n",
    "    return theta, phi\n",
    "\n",
    "def normalize_point(point):\n",
    "    # normalize a point about a sphere of radius 1.\n",
    "    R = 1\n",
    "\n",
    "    xAvg = point[:,0].mean()\n",
    "    yAvg = point[:,1].mean()\n",
    "    xy_norm = (((point - np.array([[xAvg, yAvg]])) ** 2).sum(axis=1) ** 0.5).mean()\n",
    "    diagonal_element = (R ** 0.5) / xy_norm\n",
    "    element_13 = -(R ** 0.5) * xAvg / xy_norm\n",
    "    element_23 = -(R ** 0.5) * yAvg / xy_norm\n",
    "    norm_mat = np.array([[diagonal_element, 0, element_13], [0, diagonal_element, element_23], [0, 0, 1]])\n",
    "    point = np.concatenate([point, np.ones((point.shape[0],1))], axis=1)\n",
    "    return norm_mat.dot(point.T).T\n",
    "\n",
    "def absolute_scale(images):\n",
    "    # maximum frames to lookahead to find a valid match.\n",
    "    MAX_LOOKAHEAD = 15\n",
    "\n",
    "    # threshold for theta.\n",
    "    THETA_THRES = 30\n",
    "\n",
    "    # this is the offset value of the camera from the Center of Gravity of the car.\n",
    "    # I have taken the same value as is taken in paper.\n",
    "    # The value is in meters.\n",
    "    D_COM = 0.9 \n",
    "\n",
    "    left = 0\n",
    "    right = 1\n",
    "    N = len(images)\n",
    "    curvatures = []\n",
    "\n",
    "    pbar = tqdm(total=N)\n",
    "\n",
    "    # generate a list of valid curvatures.\n",
    "    while(right < N):\n",
    "        img1 = cv2.cvtColor(images[left], cv2.COLOR_RGB2GRAY)\n",
    "        img2 = cv2.cvtColor(images[right], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        orb = cv2.ORB_create(3000)\n",
    "        FLANN_INDEX_LSH = 6\n",
    "        index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n",
    "        search_params = dict(checks=50)\n",
    "        flann = cv2.FlannBasedMatcher(indexParams=index_params, searchParams=search_params)\n",
    "\n",
    "        kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "        kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "        # Find the matches there do not have a to high distance\n",
    "        good = []\n",
    "        try:\n",
    "            for m, n in matches:\n",
    "                if m.distance < 0.8 * n.distance:\n",
    "                    good.append(m)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Get the image points form the good matches\n",
    "        q1 = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "        q2 = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "\n",
    "        # normalizing points about the sphere of radius 1.\n",
    "        q1 = normalize_point(q1)\n",
    "        q2 = normalize_point(q2)\n",
    "\n",
    "        theta, phi = find_theta_phi(q1, q2)\n",
    "        theta_deg = theta * 180 / np.pi\n",
    "\n",
    "        if abs(theta_deg) < THETA_THRES:\n",
    "            # no motion detected.\n",
    "            right += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        rho = (D_COM * np.sin(phi) - D_COM * np.sin(phi - theta)) / np.sin(phi - theta/2.)\n",
    "\n",
    "        if rho < 0:\n",
    "            left = right\n",
    "            right += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        k = 2 * np.sin(theta / 2.) / rho\n",
    "        curvatures.append({\n",
    "            \"value\": k,\n",
    "            \"left\": left,\n",
    "            \"right\": right,\n",
    "            \"rho\": rho\n",
    "        })\n",
    "\n",
    "        left = right\n",
    "        right += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"A total of {len(curvatures)} curvatures were extracted.\")\n",
    "\n",
    "    # now we select which curvatures depict a circular motion.\n",
    "    mask = []\n",
    "    for i in range(len(curvatures)-1):\n",
    "        val = abs(curvatures[i][\"value\"] - curvatures[i+1][\"value\"]) / abs(curvatures[i][\"value\"])\n",
    "        if val < 0.1:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "    \n",
    "    circulars = []\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] == 1:\n",
    "            circulars.append(curvatures[i])\n",
    "    \n",
    "    print(f\"A total of {len(circulars)} circular motions were extracted.\")\n",
    "\n",
    "    # create a list of possible absolute scales and plot a histogram.\n",
    "    rhos = []\n",
    "    for circ in circulars:\n",
    "        rho = circ[\"rho\"]\n",
    "        rhos.append(rho)\n",
    "\n",
    "    return rhos, circulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the subset frames present at 'demo/turn/'\n",
    "img_path = \"demo/turn/images/\"\n",
    "image_paths = sorted(os.listdir(img_path))\n",
    "images = []\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(img_path + \"/\" + path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 690/691 [01:40<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 354 curvatures were extracted.\n",
      "A total of 15 circular motions were extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rhos, circulars = absolute_scale(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted absolute scale: 0.27965190323954897\n"
     ]
    }
   ],
   "source": [
    "# creating a histogram of rhos.\n",
    "counts, bins = np.histogram(rhos, bins=5)\n",
    "\n",
    "# now we extract the bin value with highest frequency.\n",
    "# note that this just a heuristic to remove the affect\n",
    "# of possible outliers / wrong predictions. this method\n",
    "# can definitely be improved by using some more robust\n",
    "# techniques.\n",
    "idxs = np.argsort(counts)\n",
    "rho_prediction = bins[idxs[-1]]\n",
    "print(\"Predicted absolute scale:\", rho_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Odometry\n",
    "Visual Odometry is the process of reconstructing the camera path through visual input / video. The VO is calculated in a relative scale.\n",
    "However, since we have estimated an absolute scale we have the information to construct an odometry in the absolute scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.odometry.visual_odometry import VisualOdometry\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [01:12<00:00, 10.09it/s]"
     ]
    }
   ],
   "source": [
    "odometry = VisualOdometry(\"demo/turn\", rho_prediction)\n",
    "estimated_path = []\n",
    "\n",
    "if not os.path.exists(Path(RESULTS_DIR) / \"odometry\"):\n",
    "    os.mkdir(Path(RESULTS_DIR) / \"odometry\")\n",
    "\n",
    "pbar = tqdm(total=len(images))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    if i == 0:\n",
    "        curr_pose = np.eye(4)\n",
    "    else:\n",
    "        q1, q2 = odometry.get_matches(i)\n",
    "        transf, _, _, _ = odometry.get_pose(q1, q2)\n",
    "        curr_pose = np.matmul(curr_pose, np.linalg.inv(transf))\n",
    "        estimated_path.append((curr_pose[0, 3], curr_pose[2, 3]))\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the absolute scaled trajectory.\n",
    "estimated_path = np.array(estimated_path)\n",
    "fig = plt.figure()\n",
    "plt.title(\"Estimated Trajectory around corner.\")\n",
    "plt.scatter(estimated_path[:,0], estimated_path[:,1])\n",
    "# TODO: plot circulars.\n",
    "plt.gca().set_xlim(-100, 100)\n",
    "plt.gca().set_ylim(-100, 100)\n",
    "plt.grid(\"minor\")\n",
    "plt.xlabel(\"x (in meters)\")\n",
    "plt.ylabel(\"y (in meters)\")\n",
    "plt.savefig(\"results/odometry/vo.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated trajectory is saved in `results/odometry/vo.png`. Also, take a look at the images present in `demo/turn/images` for getting\n",
    "an idea about the actual trajectory. As we can observe, the actual trajectory (as observed visually) is very similar to the estimated one.\n",
    "In fact, the scale of the estimated trajectory is absolute. This means that we can exactly measure the location (in meters) of the car\n",
    "at any point in its journey. Also, we can correlate this frame-by-frame data along with the detection and tracking data obtained earlier\n",
    "and predict the exact coordinates of a particular pothole, thus flagging it efficiently.\n",
    "\n",
    "We can also provide this functionality in real-time by using a faster algorithm. Again, this [paper](https://drops.dagstuhl.de/opus/volltexte/2011/2950/pdf/10371.ScaramuzzaDavide.Paper.2950.pdf) by David Scaramuzza can prove to be useful in this case. He solves the problem of visual odometry in real-time, using a 1-point RANSAC algorithm that exploits the non-holonomic constraints of the car."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Reconstruction\n",
    "The final part of the pipeline can be to create a 3 dimensional reconstruction of a pothole. Structure From Motion is the most\n",
    "popular technique that is used to create 3D point clouds using multiple photos. However, our problem is of a special kind.\n",
    "We have to model the road surface which is a near-planar surface. Due to this planarity, an ambiguity arises in the calculation\n",
    "of the Fundamental Matrix. Therefore, the normal equations of SFM might not produce great results in our case\n",
    "\n",
    "\n",
    "To know more about how near-planar surfaces cause this ambiguity, this [paper](https://www.mdpi.com/1424-8220/20/6/1640) proves the same.\n",
    "The paper also derives a new unambiguous equation for calculating the fundamental matrix using the homography matrix.\n",
    "Further, this paper also provides a great recursive post-processing technique on pothole point clouds that can improve the structure\n",
    "of the reconstruction even more. Unfortunately due to time-constraint, I was unable to implement this paper on the given demo video.\n",
    "\n",
    "\n",
    "So, I used a software known as VisualSFM for dense point-cloud reconstruction for different images of the same pothole. This was comparitively easy and less complex as I was already detecting and tracking the potholes. So, using the bounding boxes I extracted the pothole (whose 3d reconstruction is to be done) from all the frames in which it was tracked. This lead to a collection of different images for every pothole, from different viewing angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>489.224061</td>\n",
       "      <td>553.976963</td>\n",
       "      <td>655.011178</td>\n",
       "      <td>641.286646</td>\n",
       "      <td>D00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>447.462523</td>\n",
       "      <td>565.912678</td>\n",
       "      <td>629.445514</td>\n",
       "      <td>662.656509</td>\n",
       "      <td>D00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>412.531444</td>\n",
       "      <td>577.951470</td>\n",
       "      <td>604.871600</td>\n",
       "      <td>682.269651</td>\n",
       "      <td>D00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>379.245617</td>\n",
       "      <td>594.214712</td>\n",
       "      <td>574.975723</td>\n",
       "      <td>705.875337</td>\n",
       "      <td>D00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>313.717821</td>\n",
       "      <td>614.624470</td>\n",
       "      <td>529.240168</td>\n",
       "      <td>740.635446</td>\n",
       "      <td>D00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  frame        xmin        ymin        xmax        ymax class\n",
       "0   0     84  489.224061  553.976963  655.011178  641.286646   D00\n",
       "1   0     85  447.462523  565.912678  629.445514  662.656509   D00\n",
       "2   0     86  412.531444  577.951470  604.871600  682.269651   D00\n",
       "3   0     87  379.245617  594.214712  574.975723  705.875337   D00\n",
       "4   0     88  313.717821  614.624470  529.240168  740.635446   D00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections_df = pd.read_csv(\"results/detection/detections.csv\")\n",
    "detections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>14</td>\n",
       "      <td>237</td>\n",
       "      <td>875.782816</td>\n",
       "      <td>479.591308</td>\n",
       "      <td>1062.847179</td>\n",
       "      <td>530.034540</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>14</td>\n",
       "      <td>238</td>\n",
       "      <td>877.961334</td>\n",
       "      <td>482.368433</td>\n",
       "      <td>1068.958056</td>\n",
       "      <td>534.545135</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>14</td>\n",
       "      <td>239</td>\n",
       "      <td>878.312603</td>\n",
       "      <td>484.732444</td>\n",
       "      <td>1073.265877</td>\n",
       "      <td>539.492747</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>14</td>\n",
       "      <td>240</td>\n",
       "      <td>873.905067</td>\n",
       "      <td>488.744706</td>\n",
       "      <td>1074.973133</td>\n",
       "      <td>545.832961</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>14</td>\n",
       "      <td>241</td>\n",
       "      <td>872.656626</td>\n",
       "      <td>490.931348</td>\n",
       "      <td>1080.101997</td>\n",
       "      <td>550.950386</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>14</td>\n",
       "      <td>242</td>\n",
       "      <td>867.299481</td>\n",
       "      <td>493.667772</td>\n",
       "      <td>1081.665809</td>\n",
       "      <td>555.824111</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>14</td>\n",
       "      <td>243</td>\n",
       "      <td>863.934573</td>\n",
       "      <td>495.758014</td>\n",
       "      <td>1084.194512</td>\n",
       "      <td>560.112318</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>14</td>\n",
       "      <td>244</td>\n",
       "      <td>862.372860</td>\n",
       "      <td>498.932020</td>\n",
       "      <td>1087.415660</td>\n",
       "      <td>566.094500</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>14</td>\n",
       "      <td>245</td>\n",
       "      <td>860.949160</td>\n",
       "      <td>501.014257</td>\n",
       "      <td>1092.116425</td>\n",
       "      <td>571.394727</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>14</td>\n",
       "      <td>246</td>\n",
       "      <td>854.624558</td>\n",
       "      <td>503.047790</td>\n",
       "      <td>1096.169465</td>\n",
       "      <td>578.865381</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>14</td>\n",
       "      <td>247</td>\n",
       "      <td>851.557221</td>\n",
       "      <td>505.486696</td>\n",
       "      <td>1101.494213</td>\n",
       "      <td>585.766628</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>14</td>\n",
       "      <td>248</td>\n",
       "      <td>845.871201</td>\n",
       "      <td>508.889143</td>\n",
       "      <td>1104.647695</td>\n",
       "      <td>594.342956</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>14</td>\n",
       "      <td>249</td>\n",
       "      <td>844.144521</td>\n",
       "      <td>514.333444</td>\n",
       "      <td>1107.495045</td>\n",
       "      <td>603.555072</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>14</td>\n",
       "      <td>250</td>\n",
       "      <td>845.289781</td>\n",
       "      <td>519.068271</td>\n",
       "      <td>1113.029786</td>\n",
       "      <td>613.015018</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>14</td>\n",
       "      <td>251</td>\n",
       "      <td>833.811855</td>\n",
       "      <td>529.143430</td>\n",
       "      <td>1112.502904</td>\n",
       "      <td>629.237757</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>14</td>\n",
       "      <td>252</td>\n",
       "      <td>827.935375</td>\n",
       "      <td>536.851883</td>\n",
       "      <td>1115.828709</td>\n",
       "      <td>642.232315</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>14</td>\n",
       "      <td>253</td>\n",
       "      <td>823.764437</td>\n",
       "      <td>544.704326</td>\n",
       "      <td>1119.484894</td>\n",
       "      <td>655.618987</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>14</td>\n",
       "      <td>254</td>\n",
       "      <td>821.511627</td>\n",
       "      <td>552.588051</td>\n",
       "      <td>1125.536649</td>\n",
       "      <td>670.393846</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>14</td>\n",
       "      <td>255</td>\n",
       "      <td>814.363431</td>\n",
       "      <td>563.042999</td>\n",
       "      <td>1129.321615</td>\n",
       "      <td>689.033255</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>14</td>\n",
       "      <td>256</td>\n",
       "      <td>807.346241</td>\n",
       "      <td>573.539931</td>\n",
       "      <td>1134.439131</td>\n",
       "      <td>708.642215</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>14</td>\n",
       "      <td>257</td>\n",
       "      <td>807.482177</td>\n",
       "      <td>582.644704</td>\n",
       "      <td>1142.459826</td>\n",
       "      <td>725.715071</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>14</td>\n",
       "      <td>258</td>\n",
       "      <td>796.734931</td>\n",
       "      <td>597.253180</td>\n",
       "      <td>1145.485297</td>\n",
       "      <td>749.073388</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>14</td>\n",
       "      <td>259</td>\n",
       "      <td>791.286642</td>\n",
       "      <td>613.065833</td>\n",
       "      <td>1152.598836</td>\n",
       "      <td>775.178938</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>14</td>\n",
       "      <td>260</td>\n",
       "      <td>777.298987</td>\n",
       "      <td>628.375289</td>\n",
       "      <td>1157.089199</td>\n",
       "      <td>804.340240</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>14</td>\n",
       "      <td>261</td>\n",
       "      <td>771.084337</td>\n",
       "      <td>645.443253</td>\n",
       "      <td>1166.311779</td>\n",
       "      <td>833.556994</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>14</td>\n",
       "      <td>262</td>\n",
       "      <td>752.454647</td>\n",
       "      <td>664.123701</td>\n",
       "      <td>1175.555663</td>\n",
       "      <td>873.371920</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>14</td>\n",
       "      <td>263</td>\n",
       "      <td>746.138669</td>\n",
       "      <td>677.964142</td>\n",
       "      <td>1190.549748</td>\n",
       "      <td>907.681680</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>14</td>\n",
       "      <td>264</td>\n",
       "      <td>727.396030</td>\n",
       "      <td>700.090308</td>\n",
       "      <td>1200.919896</td>\n",
       "      <td>958.207251</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>14</td>\n",
       "      <td>265</td>\n",
       "      <td>676.803597</td>\n",
       "      <td>727.589870</td>\n",
       "      <td>1201.829680</td>\n",
       "      <td>1022.964663</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>14</td>\n",
       "      <td>266</td>\n",
       "      <td>687.851518</td>\n",
       "      <td>738.260157</td>\n",
       "      <td>1225.874353</td>\n",
       "      <td>1050.871956</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>14</td>\n",
       "      <td>267</td>\n",
       "      <td>711.411409</td>\n",
       "      <td>752.194698</td>\n",
       "      <td>1249.751489</td>\n",
       "      <td>1072.782341</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>14</td>\n",
       "      <td>268</td>\n",
       "      <td>783.410056</td>\n",
       "      <td>768.696194</td>\n",
       "      <td>1292.526757</td>\n",
       "      <td>1087.565973</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>14</td>\n",
       "      <td>269</td>\n",
       "      <td>833.158671</td>\n",
       "      <td>786.750384</td>\n",
       "      <td>1320.437275</td>\n",
       "      <td>1097.474092</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>14</td>\n",
       "      <td>270</td>\n",
       "      <td>923.661570</td>\n",
       "      <td>810.411876</td>\n",
       "      <td>1364.422886</td>\n",
       "      <td>1106.696820</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>14</td>\n",
       "      <td>271</td>\n",
       "      <td>988.543771</td>\n",
       "      <td>844.175950</td>\n",
       "      <td>1411.543711</td>\n",
       "      <td>1115.092419</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>14</td>\n",
       "      <td>272</td>\n",
       "      <td>1009.743373</td>\n",
       "      <td>884.891687</td>\n",
       "      <td>1434.328618</td>\n",
       "      <td>1119.759872</td>\n",
       "      <td>D40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  frame         xmin        ymin         xmax         ymax class\n",
       "234  14    237   875.782816  479.591308  1062.847179   530.034540   D40\n",
       "236  14    238   877.961334  482.368433  1068.958056   534.545135   D40\n",
       "238  14    239   878.312603  484.732444  1073.265877   539.492747   D40\n",
       "239  14    240   873.905067  488.744706  1074.973133   545.832961   D40\n",
       "240  14    241   872.656626  490.931348  1080.101997   550.950386   D40\n",
       "241  14    242   867.299481  493.667772  1081.665809   555.824111   D40\n",
       "243  14    243   863.934573  495.758014  1084.194512   560.112318   D40\n",
       "245  14    244   862.372860  498.932020  1087.415660   566.094500   D40\n",
       "246  14    245   860.949160  501.014257  1092.116425   571.394727   D40\n",
       "247  14    246   854.624558  503.047790  1096.169465   578.865381   D40\n",
       "248  14    247   851.557221  505.486696  1101.494213   585.766628   D40\n",
       "250  14    248   845.871201  508.889143  1104.647695   594.342956   D40\n",
       "252  14    249   844.144521  514.333444  1107.495045   603.555072   D40\n",
       "254  14    250   845.289781  519.068271  1113.029786   613.015018   D40\n",
       "256  14    251   833.811855  529.143430  1112.502904   629.237757   D40\n",
       "258  14    252   827.935375  536.851883  1115.828709   642.232315   D40\n",
       "260  14    253   823.764437  544.704326  1119.484894   655.618987   D40\n",
       "262  14    254   821.511627  552.588051  1125.536649   670.393846   D40\n",
       "264  14    255   814.363431  563.042999  1129.321615   689.033255   D40\n",
       "266  14    256   807.346241  573.539931  1134.439131   708.642215   D40\n",
       "268  14    257   807.482177  582.644704  1142.459826   725.715071   D40\n",
       "270  14    258   796.734931  597.253180  1145.485297   749.073388   D40\n",
       "271  14    259   791.286642  613.065833  1152.598836   775.178938   D40\n",
       "272  14    260   777.298987  628.375289  1157.089199   804.340240   D40\n",
       "273  14    261   771.084337  645.443253  1166.311779   833.556994   D40\n",
       "274  14    262   752.454647  664.123701  1175.555663   873.371920   D40\n",
       "275  14    263   746.138669  677.964142  1190.549748   907.681680   D40\n",
       "276  14    264   727.396030  700.090308  1200.919896   958.207251   D40\n",
       "277  14    265   676.803597  727.589870  1201.829680  1022.964663   D40\n",
       "279  14    266   687.851518  738.260157  1225.874353  1050.871956   D40\n",
       "281  14    267   711.411409  752.194698  1249.751489  1072.782341   D40\n",
       "283  14    268   783.410056  768.696194  1292.526757  1087.565973   D40\n",
       "285  14    269   833.158671  786.750384  1320.437275  1097.474092   D40\n",
       "287  14    270   923.661570  810.411876  1364.422886  1106.696820   D40\n",
       "289  14    271   988.543771  844.175950  1411.543711  1115.092419   D40\n",
       "291  14    272  1009.743373  884.891687  1434.328618  1119.759872   D40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POTHOLE_CLASS = \"D40\"\n",
    "POTHOLE_ID = 14\n",
    "\n",
    "pothole_12 = detections_df.loc[(detections_df[\"id\"] == POTHOLE_ID) & (detections_df[\"class\"] == POTHOLE_CLASS), :]\n",
    "pothole_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 234/3844 [00:17<04:31, 13.31it/s] \n"
     ]
    }
   ],
   "source": [
    "pothole_crops = []\n",
    "\n",
    "# relax bbox to also get some surroundings.\n",
    "relax = 40\n",
    "\n",
    "cap = cv2.VideoCapture(DEMO_VIDEO)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not read the video!!!\")\n",
    "\n",
    "num_frames = 0\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "pbar = tqdm(total=length)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        if num_frames in pothole_12.loc[:, \"frame\"].to_list():\n",
    "            bbox = pothole_12.loc[pothole_12[\"frame\"] == num_frames, [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
    "            xmin, xmax, ymin, ymax = int(bbox[\"xmin\"]), int(bbox[\"xmax\"]), int(bbox[\"ymin\"]), int(bbox[\"ymax\"])\n",
    "            pothole_cropped = frame[ymin-relax:ymax+relax, xmin-relax:xmax+relax].copy()\n",
    "            pothole_crops.append(pothole_cropped)\n",
    "\n",
    "        num_frames += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "if not os.path.exists(Path(RESULTS_DIR) / \"reconstruction\"):\n",
    "    os.mkdir(Path(RESULTS_DIR) / \"reconstruction\")\n",
    "\n",
    "if not os.path.exists(Path(RESULTS_DIR) / \"reconstruction\" / \"images\"):\n",
    "    os.mkdir(Path(RESULTS_DIR) / \"reconstruction\" / \"images\")\n",
    "\n",
    "for idx,crop in enumerate(pothole_crops):\n",
    "    cv2.imwrite((Path(RESULTS_DIR) / \"reconstruction\" / \"images\" / f\"{idx}.jpg\").as_posix(), crop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting cropped images can then be used with VisualSFM or any other SFM tool.\n",
    "\n",
    "### Conclusion\n",
    "As we observed that, even by using a single video source, we can generate a lot of analysis about the surrounding. The detection\n",
    "aspect of this approach is definitely more mature. We can even say that the accuracy obtained through pothole detection and tracking\n",
    "can be at least as good as LIDAR if not better. Moreover, the overall infrastructure required to setup this is negligible and\n",
    "this can even be deployed on smartphones. The solution is completely autonomous as there is no input required from a human.\n",
    "From object detection and tracking to 3d reconstruction, everything can be done automatically using different heuristics. A human\n",
    "is needed only in a supervising capacity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fda91435a63222b52375ad7febdc25de7c9894bc34f4edc19503b4a62645f0aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
